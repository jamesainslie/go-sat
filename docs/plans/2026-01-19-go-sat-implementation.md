# go-sat Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Build a pure Go library for sentence boundary detection using wtpsplit/SaT ONNX models.

**Architecture:** Three-layer design: API layer (Segmenter), Inference layer (ONNX session pool), Tokenizer layer (SentencePiece Unigram). Context-first API with explicit errors. Session pooling for concurrent access.

**Tech Stack:** Go 1.21+, onnxruntime_go, protobuf for model parsing

---

## Task 0: Project Setup

**Files:**
- Create: `go.mod`
- Create: `go.sum`
- Create: `testdata/README.md`
- Create: `scripts/generate_golden.py`
- Download: `testdata/sentencepiece.bpe.model`

### Step 1: Initialize Go module

```bash
cd /Volumes/Development/go-sat
go mod init github.com/jamesainslie/go-sat
```

### Step 2: Add dependencies

```bash
go get github.com/yalue/onnxruntime_go@v1.13.0
go get google.golang.org/protobuf@latest
```

### Step 3: Create directory structure

```bash
mkdir -p tokenizer inference internal/proto testdata scripts cmd/sat-cli
```

### Step 4: Download tokenizer model for testing

```bash
curl -L -o testdata/sentencepiece.bpe.model \
  "https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model"
```

### Step 5: Create golden file generator script

Create `scripts/generate_golden.py`:

```python
#!/usr/bin/env python3
"""Generate golden test files for go-sat tokenizer validation."""

import json
from transformers import AutoTokenizer

def main():
    tok = AutoTokenizer.from_pretrained("xlm-roberta-base")

    test_cases = [
        "Hello",
        "Hello world",
        "Hello world.",
        "I want to",
        "Thank you very much.",
        "This is a test sentence.",
        "The quick brown fox jumps over the lazy dog.",
        "",  # empty string
        "caf√©",  # non-ASCII
        "‰Ω†Â•Ω‰∏ñÁïå",  # Chinese
        "üéâ",  # emoji
    ]

    results = []
    for text in test_cases:
        enc = tok(text, return_offsets_mapping=True, add_special_tokens=False)
        results.append({
            "input": text,
            "token_ids": enc["input_ids"],
            "offsets": enc["offset_mapping"],
            "tokens": tok.convert_ids_to_tokens(enc["input_ids"]),
        })

    with open("testdata/tokenizer_golden.json", "w") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)

    print(f"Generated {len(results)} test cases")

if __name__ == "__main__":
    main()
```

### Step 6: Run golden file generator

```bash
cd /Volumes/Development/go-sat
python3 scripts/generate_golden.py
```

Expected: Creates `testdata/tokenizer_golden.json`

### Step 7: Create testdata README

Create `testdata/README.md`:

```markdown
# Test Data

## Files

- `sentencepiece.bpe.model` - XLM-RoBERTa tokenizer model from HuggingFace
- `tokenizer_golden.json` - Expected tokenizer outputs generated by Python

## Regenerating Golden Files

Requires Python with transformers:

```bash
pip install transformers
python3 scripts/generate_golden.py
```
```

### Step 8: Commit setup

```bash
git add -A
git commit -m "chore: project setup with dependencies and test data

- Initialize go module
- Add onnxruntime_go and protobuf dependencies
- Download XLM-RoBERTa tokenizer model
- Create golden file generator script
- Generate tokenizer test cases from Python

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 1: Protobuf Model Loading

**Files:**
- Create: `internal/proto/sentencepiece_model.proto`
- Create: `internal/proto/sentencepiece_model.pb.go` (generated)
- Create: `tokenizer/model.go`
- Create: `tokenizer/model_test.go`

### Step 1: Write failing test for model loading

Create `tokenizer/model_test.go`:

```go
package tokenizer

import (
	"testing"
)

func TestLoadModel(t *testing.T) {
	model, err := LoadModel("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("LoadModel failed: %v", err)
	}

	// XLM-RoBERTa has 250002 vocab size
	if len(model.Pieces) == 0 {
		t.Error("expected non-empty pieces")
	}

	// Check special tokens exist
	if model.Pieces[0].Piece != "<s>" {
		t.Errorf("expected piece[0] = <s>, got %s", model.Pieces[0].Piece)
	}
	if model.Pieces[1].Piece != "<pad>" {
		t.Errorf("expected piece[1] = <pad>, got %s", model.Pieces[1].Piece)
	}
	if model.Pieces[2].Piece != "</s>" {
		t.Errorf("expected piece[2] = </s>, got %s", model.Pieces[2].Piece)
	}
	if model.Pieces[3].Piece != "<unk>" {
		t.Errorf("expected piece[3] = <unk>, got %s", model.Pieces[3].Piece)
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./tokenizer -v -run TestLoadModel
```

Expected: FAIL - `LoadModel` undefined

### Step 3: Create protobuf definition

Create `internal/proto/sentencepiece_model.proto`:

```protobuf
syntax = "proto2";

package sentencepiece;

option go_package = "github.com/jamesainslie/go-sat/internal/proto";

message TrainerSpec {
  enum ModelType {
    UNIGRAM = 1;
    BPE = 2;
    WORD = 3;
    CHAR = 4;
  }
  optional ModelType model_type = 3 [default = UNIGRAM];
  optional string unk_piece = 10 [default = "<unk>"];
  optional string bos_piece = 11 [default = "<s>"];
  optional string eos_piece = 12 [default = "</s>"];
  optional string pad_piece = 13 [default = "<pad>"];
}

message NormalizerSpec {
  optional string name = 1;
  optional string precompiled_charsmap = 2;
  optional bool add_dummy_prefix = 3 [default = true];
  optional bool remove_extra_whitespaces = 4 [default = true];
  optional bool escape_whitespaces = 5 [default = true];
}

message ModelProto {
  message SentencePiece {
    optional string piece = 1;
    optional float score = 2;
    enum Type {
      NORMAL = 1;
      UNKNOWN = 2;
      CONTROL = 3;
      USER_DEFINED = 4;
      UNUSED = 5;
      BYTE = 6;
    }
    optional Type type = 3 [default = NORMAL];
  }
  repeated SentencePiece pieces = 1;
  optional TrainerSpec trainer_spec = 2;
  optional NormalizerSpec normalizer_spec = 3;
}
```

### Step 4: Generate Go code from protobuf

```bash
protoc --go_out=. --go_opt=paths=source_relative \
  internal/proto/sentencepiece_model.proto
```

Expected: Creates `internal/proto/sentencepiece_model.pb.go`

### Step 5: Implement model loading

Create `tokenizer/model.go`:

```go
package tokenizer

import (
	"fmt"
	"os"

	"google.golang.org/protobuf/proto"

	pb "github.com/jamesainslie/go-sat/internal/proto"
)

// Model represents a loaded SentencePiece model.
type Model struct {
	Pieces        []*pb.ModelProto_SentencePiece
	TrainerSpec   *pb.TrainerSpec
	NormalizerSpec *pb.NormalizerSpec
}

// LoadModel loads a SentencePiece model from a .model file.
func LoadModel(path string) (*Model, error) {
	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("reading model file: %w", err)
	}

	var modelProto pb.ModelProto
	if err := proto.Unmarshal(data, &modelProto); err != nil {
		return nil, fmt.Errorf("parsing protobuf: %w", err)
	}

	return &Model{
		Pieces:         modelProto.Pieces,
		TrainerSpec:    modelProto.TrainerSpec,
		NormalizerSpec: modelProto.NormalizerSpec,
	}, nil
}
```

### Step 6: Run test to verify it passes

```bash
go test ./tokenizer -v -run TestLoadModel
```

Expected: PASS

### Step 7: Add test for model type verification

Add to `tokenizer/model_test.go`:

```go
func TestLoadModel_IsUnigram(t *testing.T) {
	model, err := LoadModel("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("LoadModel failed: %v", err)
	}

	if model.TrainerSpec == nil {
		t.Fatal("expected trainer_spec to be present")
	}

	// Despite filename, XLM-RoBERTa uses UNIGRAM
	modelType := model.TrainerSpec.GetModelType()
	if modelType != pb.TrainerSpec_UNIGRAM {
		t.Errorf("expected UNIGRAM model type, got %v", modelType)
	}
}
```

### Step 8: Run tests

```bash
go test ./tokenizer -v
```

Expected: PASS

### Step 9: Commit

```bash
git add -A
git commit -m "feat(tokenizer): add protobuf model loading

- Define SentencePiece protobuf schema
- Implement LoadModel to parse .model files
- Verify XLM-RoBERTa model uses UNIGRAM type

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 2: Tokenizer Core Structure

**Files:**
- Create: `tokenizer/tokenizer.go`
- Modify: `tokenizer/model_test.go` ‚Üí `tokenizer/tokenizer_test.go`

### Step 1: Write failing test for tokenizer creation

Create `tokenizer/tokenizer_test.go`:

```go
package tokenizer

import (
	"testing"
)

func TestNew(t *testing.T) {
	tok, err := New("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("New failed: %v", err)
	}
	defer tok.Close()

	// Check vocab size (XLM-RoBERTa = 250002)
	if tok.VocabSize() < 250000 {
		t.Errorf("expected vocab size >= 250000, got %d", tok.VocabSize())
	}

	// Check special token IDs
	if tok.BOSID() != 0 {
		t.Errorf("expected BOS ID = 0, got %d", tok.BOSID())
	}
	if tok.PadID() != 1 {
		t.Errorf("expected PAD ID = 1, got %d", tok.PadID())
	}
	if tok.EOSID() != 2 {
		t.Errorf("expected EOS ID = 2, got %d", tok.EOSID())
	}
	if tok.UnkID() != 3 {
		t.Errorf("expected UNK ID = 3, got %d", tok.UnkID())
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./tokenizer -v -run TestNew
```

Expected: FAIL - `New` undefined

### Step 3: Implement tokenizer structure

Create `tokenizer/tokenizer.go`:

```go
package tokenizer

import (
	"fmt"

	pb "github.com/jamesainslie/go-sat/internal/proto"
)

// Tokenizer implements XLM-RoBERTa compatible SentencePiece Unigram tokenization.
type Tokenizer struct {
	pieces      map[string]int32   // token string ‚Üí ID
	scores      map[string]float32 // token string ‚Üí log probability
	idToPiece   []string           // ID ‚Üí token string
	pieceToType map[string]pb.ModelProto_SentencePiece_Type

	bosID int32
	padID int32
	eosID int32
	unkID int32

	maxTokenLen int
}

// TokenInfo represents a token with its position in the original text.
type TokenInfo struct {
	ID    int32
	Text  string
	Start int // byte offset in original text
	End   int // byte offset in original text
}

// New loads a tokenizer from a SentencePiece .model file.
func New(modelPath string) (*Tokenizer, error) {
	model, err := LoadModel(modelPath)
	if err != nil {
		return nil, fmt.Errorf("loading model: %w", err)
	}

	t := &Tokenizer{
		pieces:      make(map[string]int32),
		scores:      make(map[string]float32),
		idToPiece:   make([]string, len(model.Pieces)),
		pieceToType: make(map[string]pb.ModelProto_SentencePiece_Type),
		bosID:       -1,
		padID:       -1,
		eosID:       -1,
		unkID:       -1,
	}

	for i, piece := range model.Pieces {
		id := int32(i)
		pieceStr := piece.GetPiece()

		t.pieces[pieceStr] = id
		t.scores[pieceStr] = piece.GetScore()
		t.idToPiece[i] = pieceStr
		t.pieceToType[pieceStr] = piece.GetType()

		// Track max token length for optimization
		if len(pieceStr) > t.maxTokenLen {
			t.maxTokenLen = len(pieceStr)
		}

		// Identify special tokens
		switch piece.GetType() {
		case pb.ModelProto_SentencePiece_CONTROL:
			switch pieceStr {
			case "<s>":
				t.bosID = id
			case "</s>":
				t.eosID = id
			case "<pad>":
				t.padID = id
			}
		case pb.ModelProto_SentencePiece_UNKNOWN:
			t.unkID = id
		}
	}

	return t, nil
}

// Close releases tokenizer resources.
func (t *Tokenizer) Close() error {
	return nil
}

// VocabSize returns the vocabulary size.
func (t *Tokenizer) VocabSize() int {
	return len(t.idToPiece)
}

// BOSID returns the beginning-of-sentence token ID.
func (t *Tokenizer) BOSID() int32 { return t.bosID }

// PadID returns the padding token ID.
func (t *Tokenizer) PadID() int32 { return t.padID }

// EOSID returns the end-of-sentence token ID.
func (t *Tokenizer) EOSID() int32 { return t.eosID }

// UnkID returns the unknown token ID.
func (t *Tokenizer) UnkID() int32 { return t.unkID }
```

### Step 4: Run test to verify it passes

```bash
go test ./tokenizer -v -run TestNew
```

Expected: PASS

### Step 5: Commit

```bash
git add -A
git commit -m "feat(tokenizer): add core tokenizer structure

- Implement Tokenizer with vocab maps
- Load pieces, scores, and special token IDs
- Track max token length for Viterbi optimization

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 3: Text Normalization

**Files:**
- Create: `tokenizer/normalize.go`
- Create: `tokenizer/normalize_test.go`

### Step 1: Write failing test for normalization

Create `tokenizer/normalize_test.go`:

```go
package tokenizer

import (
	"testing"
)

func TestNormalize(t *testing.T) {
	tests := []struct {
		input    string
		expected string
	}{
		{"Hello", "‚ñÅHello"},
		{"Hello world", "‚ñÅHello‚ñÅworld"},
		{"  spaces  ", "‚ñÅspaces"},
		{"", ""},
	}

	for _, tc := range tests {
		got := normalize(tc.input)
		if got != tc.expected {
			t.Errorf("normalize(%q) = %q, want %q", tc.input, got, tc.expected)
		}
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./tokenizer -v -run TestNormalize
```

Expected: FAIL - `normalize` undefined

### Step 3: Implement normalization

Create `tokenizer/normalize.go`:

```go
package tokenizer

import (
	"strings"
	"unicode"
)

const sentencePieceSpace = '‚ñÅ' // U+2581 LOWER ONE EIGHTH BLOCK

// normalize prepares text for tokenization following XLM-RoBERTa conventions.
// - Adds dummy prefix (space at start)
// - Replaces spaces with ‚ñÅ
// - Normalizes whitespace
func normalize(text string) string {
	if text == "" {
		return ""
	}

	// Normalize whitespace: collapse runs, trim
	var builder strings.Builder
	prevSpace := true // treat start as after space to add dummy prefix

	for _, r := range text {
		if unicode.IsSpace(r) {
			if !prevSpace {
				builder.WriteRune(sentencePieceSpace)
				prevSpace = true
			}
		} else {
			if prevSpace && builder.Len() == 0 {
				// Add dummy prefix at start
				builder.WriteRune(sentencePieceSpace)
			}
			builder.WriteRune(r)
			prevSpace = false
		}
	}

	return builder.String()
}
```

### Step 4: Run test to verify it passes

```bash
go test ./tokenizer -v -run TestNormalize
```

Expected: PASS

### Step 5: Commit

```bash
git add -A
git commit -m "feat(tokenizer): add text normalization

- Prepend dummy prefix (‚ñÅ) per XLM-RoBERTa convention
- Replace spaces with ‚ñÅ (U+2581)
- Normalize whitespace runs

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 4: Unigram Viterbi Algorithm

**Files:**
- Create: `tokenizer/unigram.go`
- Modify: `tokenizer/tokenizer_test.go`

### Step 1: Write failing test for encoding

Add to `tokenizer/tokenizer_test.go`:

```go
func TestTokenizer_EncodeIDs_Simple(t *testing.T) {
	tok, err := New("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("New failed: %v", err)
	}
	defer tok.Close()

	// Simple case - just verify we get some tokens
	ids := tok.EncodeIDs("Hello")
	if len(ids) == 0 {
		t.Error("expected non-empty token IDs")
	}

	// All IDs should be valid
	for i, id := range ids {
		if id < 0 || int(id) >= tok.VocabSize() {
			t.Errorf("token %d: invalid ID %d", i, id)
		}
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./tokenizer -v -run TestTokenizer_EncodeIDs_Simple
```

Expected: FAIL - `EncodeIDs` undefined

### Step 3: Implement Viterbi algorithm

Create `tokenizer/unigram.go`:

```go
package tokenizer

import (
	"math"
)

const negInf = -1e9

// EncodeIDs returns token IDs for the input text.
func (t *Tokenizer) EncodeIDs(text string) []int32 {
	tokens := t.Encode(text)
	ids := make([]int32, len(tokens))
	for i, tok := range tokens {
		ids[i] = tok.ID
	}
	return ids
}

// Encode tokenizes text using Viterbi algorithm, returning tokens with offsets.
func (t *Tokenizer) Encode(text string) []TokenInfo {
	if text == "" {
		return nil
	}

	// Normalize text (add ‚ñÅ prefix, replace spaces)
	normalized := normalize(text)
	if normalized == "" {
		return nil
	}

	runes := []rune(normalized)
	n := len(runes)

	// best[i] = best log probability to tokenize runes[0:i]
	best := make([]float64, n+1)
	// parent[i] = start position of the token ending at position i
	parent := make([]int, n+1)
	// tokenAt[i] = the token string ending at position i
	tokenAt := make([]string, n+1)

	for i := 1; i <= n; i++ {
		best[i] = negInf
		parent[i] = -1
	}

	// Dynamic programming: find best tokenization
	for i := 1; i <= n; i++ {
		// Try all possible tokens ending at position i
		maxLen := t.maxTokenLen
		if maxLen > i {
			maxLen = i
		}

		for length := 1; length <= maxLen; length++ {
			j := i - length
			substr := string(runes[j:i])

			score, exists := t.scores[substr]
			if !exists {
				continue
			}

			candidate := best[j] + float64(score)
			if candidate > best[i] {
				best[i] = candidate
				parent[i] = j
				tokenAt[i] = substr
			}
		}

		// If no valid token found, use unknown token for single character
		if best[i] == negInf {
			// Try byte fallback or unknown token
			best[i] = best[i-1] + float64(t.scores[t.idToPiece[t.unkID]])
			parent[i] = i - 1
			tokenAt[i] = string(runes[i-1 : i])
		}
	}

	// Backtrack to get tokens
	var tokens []TokenInfo
	pos := n
	for pos > 0 {
		start := parent[pos]
		tokenStr := tokenAt[pos]

		id, ok := t.pieces[tokenStr]
		if !ok {
			id = t.unkID
		}

		tokens = append(tokens, TokenInfo{
			ID:    id,
			Text:  tokenStr,
			Start: start,
			End:   pos,
		})
		pos = start
	}

	// Reverse to get correct order
	for i, j := 0, len(tokens)-1; i < j; i, j = i+1, j-1 {
		tokens[i], tokens[j] = tokens[j], tokens[i]
	}

	return tokens
}
```

### Step 4: Run test to verify it passes

```bash
go test ./tokenizer -v -run TestTokenizer_EncodeIDs_Simple
```

Expected: PASS

### Step 5: Commit

```bash
git add -A
git commit -m "feat(tokenizer): implement Unigram Viterbi encoding

- Dynamic programming for optimal tokenization
- Maximize sum of log probabilities
- Backtrack to recover token sequence
- Handle unknown tokens with fallback

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 5: Golden File Validation

**Files:**
- Modify: `tokenizer/tokenizer_test.go`

### Step 1: Write test against golden files

Add to `tokenizer/tokenizer_test.go`:

```go
import (
	"encoding/json"
	"os"
	"testing"
)

type goldenCase struct {
	Input    string  `json:"input"`
	TokenIDs []int32 `json:"token_ids"`
	Offsets  [][2]int `json:"offsets"`
	Tokens   []string `json:"tokens"`
}

func loadGoldenCases(t *testing.T) []goldenCase {
	t.Helper()
	data, err := os.ReadFile("../testdata/tokenizer_golden.json")
	if err != nil {
		t.Fatalf("loading golden file: %v", err)
	}
	var cases []goldenCase
	if err := json.Unmarshal(data, &cases); err != nil {
		t.Fatalf("parsing golden file: %v", err)
	}
	return cases
}

func TestTokenizer_EncodeIDs_Golden(t *testing.T) {
	tok, err := New("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("New failed: %v", err)
	}
	defer tok.Close()

	cases := loadGoldenCases(t)
	for _, tc := range cases {
		t.Run(tc.Input, func(t *testing.T) {
			if tc.Input == "" {
				// Skip empty string - handled separately
				return
			}

			got := tok.EncodeIDs(tc.Input)

			if len(got) != len(tc.TokenIDs) {
				t.Errorf("length mismatch: got %d tokens, want %d", len(got), len(tc.TokenIDs))
				t.Logf("got:  %v", got)
				t.Logf("want: %v", tc.TokenIDs)
				return
			}

			for i := range got {
				if got[i] != tc.TokenIDs[i] {
					t.Errorf("token %d: got ID %d, want %d", i, got[i], tc.TokenIDs[i])
				}
			}
		})
	}
}
```

### Step 2: Run test

```bash
go test ./tokenizer -v -run TestTokenizer_EncodeIDs_Golden
```

Expected: May PASS or FAIL depending on tokenizer accuracy. If FAIL, debug and fix.

### Step 3: Debug and fix any mismatches

If tests fail, common issues:
- Offset calculation (byte vs rune)
- Special handling of certain characters
- Byte fallback tokens

### Step 4: Commit when passing

```bash
git add -A
git commit -m "test(tokenizer): validate against Python golden files

- Load golden test cases from JSON
- Compare token IDs with Python transformers output
- All test cases passing

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 6: Decode Implementation

**Files:**
- Modify: `tokenizer/tokenizer.go`
- Modify: `tokenizer/tokenizer_test.go`

### Step 1: Write failing test for decode

Add to `tokenizer/tokenizer_test.go`:

```go
func TestTokenizer_Decode(t *testing.T) {
	tok, err := New("../testdata/sentencepiece.bpe.model")
	if err != nil {
		t.Fatalf("New failed: %v", err)
	}
	defer tok.Close()

	tests := []struct {
		input    string
		expected string
	}{
		{"Hello", "Hello"},
		{"Hello world", "Hello world"},
	}

	for _, tc := range tests {
		ids := tok.EncodeIDs(tc.input)
		got := tok.Decode(ids)
		if got != tc.expected {
			t.Errorf("Decode(Encode(%q)) = %q, want %q", tc.input, got, tc.expected)
		}
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./tokenizer -v -run TestTokenizer_Decode
```

Expected: FAIL - `Decode` undefined

### Step 3: Implement Decode

Add to `tokenizer/tokenizer.go`:

```go
import "strings"

// Decode converts token IDs back to text.
func (t *Tokenizer) Decode(ids []int32) string {
	var builder strings.Builder

	for _, id := range ids {
		if id < 0 || int(id) >= len(t.idToPiece) {
			continue
		}
		piece := t.idToPiece[id]

		// Skip control tokens
		if t.pieceToType[piece] == pb.ModelProto_SentencePiece_CONTROL {
			continue
		}

		builder.WriteString(piece)
	}

	// Convert ‚ñÅ back to spaces and trim leading space
	result := builder.String()
	result = strings.ReplaceAll(result, string(sentencePieceSpace), " ")
	result = strings.TrimPrefix(result, " ")

	return result
}
```

### Step 4: Run test to verify it passes

```bash
go test ./tokenizer -v -run TestTokenizer_Decode
```

Expected: PASS

### Step 5: Commit

```bash
git add -A
git commit -m "feat(tokenizer): implement Decode

- Convert token IDs back to text
- Replace ‚ñÅ with spaces
- Skip control tokens

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 7: ONNX Session Wrapper

**Files:**
- Create: `inference/session.go`
- Create: `inference/session_test.go`

### Step 1: Write failing test for session creation

Create `inference/session_test.go`:

```go
package inference

import (
	"testing"
)

func TestNewSession(t *testing.T) {
	// Skip if no model available
	modelPath := "../testdata/model_optimized.onnx"

	session, err := NewSession(modelPath)
	if err != nil {
		t.Skipf("Skipping: model not available at %s: %v", modelPath, err)
	}
	defer session.Close()

	if session == nil {
		t.Error("expected non-nil session")
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test ./inference -v -run TestNewSession
```

Expected: FAIL - `NewSession` undefined

### Step 3: Implement session wrapper

Create `inference/session.go`:

```go
package inference

import (
	"context"
	"fmt"
	"os"
	"sync"

	ort "github.com/yalue/onnxruntime_go"
)

var (
	ortEnvOnce sync.Once
	ortEnvErr  error
)

// initORT initializes ONNX Runtime environment once.
func initORT() error {
	ortEnvOnce.Do(func() {
		ortEnvErr = ort.InitializeEnvironment()
	})
	return ortEnvErr
}

// Session wraps an ONNX Runtime session for SaT inference.
type Session struct {
	session *ort.DynamicAdvancedSession
}

// NewSession creates a new ONNX session from a model file.
func NewSession(modelPath string) (*Session, error) {
	// Check file exists
	if _, err := os.Stat(modelPath); err != nil {
		return nil, fmt.Errorf("model file: %w", err)
	}

	if err := initORT(); err != nil {
		return nil, fmt.Errorf("initializing ONNX runtime: %w", err)
	}

	// Create session options
	options, err := ort.NewSessionOptions()
	if err != nil {
		return nil, fmt.Errorf("creating session options: %w", err)
	}
	defer options.Destroy()

	// Define input/output names (from model inspection)
	inputNames := []string{"input_ids", "attention_mask"}
	outputNames := []string{"logits"}

	session, err := ort.NewDynamicAdvancedSession(
		modelPath,
		inputNames,
		outputNames,
		options,
	)
	if err != nil {
		return nil, fmt.Errorf("creating session: %w", err)
	}

	return &Session{session: session}, nil
}

// Infer runs the model on tokenized input, returns per-token logits.
func (s *Session) Infer(ctx context.Context, inputIDs, attentionMask []int64) ([]float32, error) {
	// Check context before expensive operation
	select {
	case <-ctx.Done():
		return nil, ctx.Err()
	default:
	}

	batchSize := int64(1)
	seqLen := int64(len(inputIDs))

	// Create input tensors
	inputIDsTensor, err := ort.NewTensor(
		ort.NewShape(batchSize, seqLen),
		inputIDs,
	)
	if err != nil {
		return nil, fmt.Errorf("creating input_ids tensor: %w", err)
	}
	defer inputIDsTensor.Destroy()

	attentionMaskTensor, err := ort.NewTensor(
		ort.NewShape(batchSize, seqLen),
		attentionMask,
	)
	if err != nil {
		return nil, fmt.Errorf("creating attention_mask tensor: %w", err)
	}
	defer attentionMaskTensor.Destroy()

	// Run inference
	outputs, err := s.session.Run([]ort.ArbitraryTensor{
		inputIDsTensor,
		attentionMaskTensor,
	})
	if err != nil {
		return nil, fmt.Errorf("running inference: %w", err)
	}

	// Extract logits from output
	logitsTensor, ok := outputs[0].(*ort.Tensor[float32])
	if !ok {
		return nil, fmt.Errorf("unexpected output tensor type")
	}
	defer logitsTensor.Destroy()

	// Copy output data
	logits := make([]float32, seqLen)
	outputData := logitsTensor.GetData()
	for i := int64(0); i < seqLen; i++ {
		logits[i] = outputData[i]
	}

	return logits, nil
}

// Close releases ONNX resources.
func (s *Session) Close() error {
	if s.session != nil {
		return s.session.Destroy()
	}
	return nil
}
```

### Step 4: Run test

```bash
go test ./inference -v -run TestNewSession
```

Expected: PASS (or SKIP if model not available)

### Step 5: Commit

```bash
git add -A
git commit -m "feat(inference): add ONNX session wrapper

- Initialize ONNX Runtime environment
- Create session from model file
- Implement Infer with input tensors
- Handle context cancellation

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 8: Session Pool

**Files:**
- Create: `inference/pool.go`
- Create: `inference/pool_test.go`

### Step 1: Write failing test for pool

Create `inference/pool_test.go`:

```go
package inference

import (
	"context"
	"testing"
	"time"
)

func TestPool_AcquireRelease(t *testing.T) {
	modelPath := "../testdata/model_optimized.onnx"

	pool, err := NewPool(modelPath, 2)
	if err != nil {
		t.Skipf("Skipping: %v", err)
	}
	defer pool.Close()

	ctx := context.Background()

	// Acquire first session
	s1, err := pool.Acquire(ctx)
	if err != nil {
		t.Fatalf("Acquire 1 failed: %v", err)
	}

	// Acquire second session
	s2, err := pool.Acquire(ctx)
	if err != nil {
		t.Fatalf("Acquire 2 failed: %v", err)
	}

	// Third acquire should block - test with timeout
	ctx3, cancel := context.WithTimeout(ctx, 50*time.Millisecond)
	defer cancel()

	_, err = pool.Acquire(ctx3)
	if err != context.DeadlineExceeded {
		t.Errorf("expected DeadlineExceeded, got %v", err)
	}

	// Release one and acquire again should work
	pool.Release(s1)

	s3, err := pool.Acquire(ctx)
	if err != nil {
		t.Fatalf("Acquire 3 failed: %v", err)
	}

	pool.Release(s2)
	pool.Release(s3)
}
```

### Step 2: Run test to verify it fails

```bash
go test ./inference -v -run TestPool
```

Expected: FAIL - `NewPool` undefined

### Step 3: Implement pool

Create `inference/pool.go`:

```go
package inference

import (
	"context"
	"fmt"
	"sync"
)

// Pool manages a pool of ONNX sessions for concurrent inference.
type Pool struct {
	sessions  chan *Session
	modelPath string
	size      int
	mu        sync.Mutex
	closed    bool
}

// NewPool creates a pool of n ONNX sessions.
func NewPool(modelPath string, size int) (*Pool, error) {
	if size <= 0 {
		size = 1
	}

	pool := &Pool{
		sessions:  make(chan *Session, size),
		modelPath: modelPath,
		size:      size,
	}

	// Pre-create all sessions
	for i := 0; i < size; i++ {
		session, err := NewSession(modelPath)
		if err != nil {
			// Clean up already created sessions
			pool.Close()
			return nil, fmt.Errorf("creating session %d: %w", i, err)
		}
		pool.sessions <- session
	}

	return pool, nil
}

// Acquire gets a session from the pool, blocking if none available.
// Respects context cancellation.
func (p *Pool) Acquire(ctx context.Context) (*Session, error) {
	select {
	case session := <-p.sessions:
		return session, nil
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}

// Release returns a session to the pool.
func (p *Pool) Release(s *Session) {
	if s == nil {
		return
	}

	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		s.Close()
		return
	}
	p.mu.Unlock()

	select {
	case p.sessions <- s:
	default:
		// Pool is full, close session
		s.Close()
	}
}

// Close closes all sessions in the pool.
func (p *Pool) Close() error {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		return nil
	}
	p.closed = true
	p.mu.Unlock()

	close(p.sessions)

	var lastErr error
	for session := range p.sessions {
		if err := session.Close(); err != nil {
			lastErr = err
		}
	}

	return lastErr
}

// Size returns the pool size.
func (p *Pool) Size() int {
	return p.size
}
```

### Step 4: Run test

```bash
go test ./inference -v -run TestPool
```

Expected: PASS (or SKIP if model not available)

### Step 5: Commit

```bash
git add -A
git commit -m "feat(inference): add session pool

- Pre-create configurable number of sessions
- Acquire/Release with channel-based pooling
- Respect context cancellation on Acquire
- Clean shutdown with Close

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 9: Segmenter API - Errors and Options

**Files:**
- Create: `errors.go`
- Create: `options.go`

### Step 1: Create errors

Create `errors.go`:

```go
package sat

import "errors"

// Sentinel errors for conditions callers may need to handle differently.
var (
	// ErrModelNotFound indicates the model file does not exist.
	ErrModelNotFound = errors.New("sat: model file not found")

	// ErrInvalidModel indicates the model file exists but is malformed.
	ErrInvalidModel = errors.New("sat: invalid model format")

	// ErrTokenizerFailed indicates tokenizer initialization failed.
	ErrTokenizerFailed = errors.New("sat: tokenizer initialization failed")
)
```

### Step 2: Create options

Create `options.go`:

```go
package sat

import (
	"log/slog"
	"runtime"
)

// Option configures a Segmenter.
type Option func(*config)

type config struct {
	threshold float32
	poolSize  int
	logger    *slog.Logger
}

func defaultConfig() config {
	return config{
		threshold: 0.025,
		poolSize:  runtime.NumCPU(),
		logger:    slog.Default(),
	}
}

// WithThreshold sets the boundary detection threshold (default: 0.025).
func WithThreshold(t float32) Option {
	return func(c *config) {
		c.threshold = t
	}
}

// WithPoolSize sets the ONNX session pool size (default: runtime.NumCPU()).
func WithPoolSize(n int) Option {
	return func(c *config) {
		if n > 0 {
			c.poolSize = n
		}
	}
}

// WithLogger sets the logger (default: slog.Default()).
func WithLogger(l *slog.Logger) Option {
	return func(c *config) {
		if l != nil {
			c.logger = l
		}
	}
}
```

### Step 3: Commit

```bash
git add -A
git commit -m "feat: add sentinel errors and options

- Define ErrModelNotFound, ErrInvalidModel, ErrTokenizerFailed
- Implement functional options pattern
- WithThreshold, WithPoolSize, WithLogger

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 10: Segmenter Core Implementation

**Files:**
- Create: `sat.go`
- Create: `sat_test.go`

### Step 1: Write failing test for Segmenter creation

Create `sat_test.go`:

```go
package sat

import (
	"context"
	"testing"
)

func TestNew(t *testing.T) {
	modelPath := "testdata/model_optimized.onnx"
	tokenizerPath := "testdata/sentencepiece.bpe.model"

	seg, err := New(modelPath, tokenizerPath)
	if err != nil {
		t.Skipf("Skipping: %v", err)
	}
	defer seg.Close()

	if seg == nil {
		t.Error("expected non-nil segmenter")
	}
}

func TestSegmenter_IsComplete_Empty(t *testing.T) {
	modelPath := "testdata/model_optimized.onnx"
	tokenizerPath := "testdata/sentencepiece.bpe.model"

	seg, err := New(modelPath, tokenizerPath)
	if err != nil {
		t.Skipf("Skipping: %v", err)
	}
	defer seg.Close()

	ctx := context.Background()
	complete, confidence, err := seg.IsComplete(ctx, "")
	if err != nil {
		t.Fatalf("IsComplete failed: %v", err)
	}

	if complete {
		t.Error("expected empty string to be incomplete")
	}
	if confidence != 0.0 {
		t.Errorf("expected confidence 0.0, got %f", confidence)
	}
}
```

### Step 2: Run test to verify it fails

```bash
go test -v -run TestNew
```

Expected: FAIL - `New` undefined

### Step 3: Implement Segmenter

Create `sat.go`:

```go
package sat

import (
	"context"
	"errors"
	"fmt"
	"log/slog"
	"math"
	"os"

	"github.com/jamesainslie/go-sat/inference"
	"github.com/jamesainslie/go-sat/tokenizer"
)

// Segmenter detects sentence boundaries using wtpsplit/SaT ONNX models.
// It is safe for concurrent use.
type Segmenter struct {
	tokenizer *tokenizer.Tokenizer
	pool      *inference.Pool
	threshold float32
	logger    *slog.Logger
}

// New creates a Segmenter with the specified model files.
func New(modelPath, tokenizerPath string, opts ...Option) (*Segmenter, error) {
	cfg := defaultConfig()
	for _, opt := range opts {
		opt(&cfg)
	}

	// Check model file exists
	if _, err := os.Stat(modelPath); err != nil {
		if errors.Is(err, os.ErrNotExist) {
			return nil, fmt.Errorf("%w: %s", ErrModelNotFound, modelPath)
		}
		return nil, fmt.Errorf("checking model file: %w", err)
	}

	// Load tokenizer
	tok, err := tokenizer.New(tokenizerPath)
	if err != nil {
		if errors.Is(err, os.ErrNotExist) {
			return nil, fmt.Errorf("%w: %s", ErrTokenizerFailed, tokenizerPath)
		}
		return nil, fmt.Errorf("%w: %v", ErrTokenizerFailed, err)
	}

	// Create session pool
	pool, err := inference.NewPool(modelPath, cfg.poolSize)
	if err != nil {
		tok.Close()
		return nil, fmt.Errorf("%w: %v", ErrInvalidModel, err)
	}

	return &Segmenter{
		tokenizer: tok,
		pool:      pool,
		threshold: cfg.threshold,
		logger:    cfg.logger,
	}, nil
}

// IsComplete returns whether text appears to be a complete sentence.
func (s *Segmenter) IsComplete(ctx context.Context, text string) (complete bool, confidence float32, err error) {
	if text == "" {
		return false, 0.0, nil
	}

	// Tokenize
	tokens := s.tokenizer.Encode(text)
	if len(tokens) == 0 {
		return false, 0.0, nil
	}

	// Prepare model input
	inputIDs := make([]int64, len(tokens))
	attentionMask := make([]int64, len(tokens))
	for i, t := range tokens {
		inputIDs[i] = int64(t.ID)
		attentionMask[i] = 1
	}

	// Acquire session from pool
	session, err := s.pool.Acquire(ctx)
	if err != nil {
		return false, 0, err
	}
	defer s.pool.Release(session)

	// Run inference
	logits, err := session.Infer(ctx, inputIDs, attentionMask)
	if err != nil {
		return false, 0, fmt.Errorf("inference: %w", err)
	}

	// Check last token's boundary probability
	lastLogit := logits[len(logits)-1]
	prob := sigmoid(lastLogit)

	complete = prob > s.threshold
	return complete, prob, nil
}

// Segment splits text into sentences.
func (s *Segmenter) Segment(ctx context.Context, text string) ([]string, error) {
	if text == "" {
		return nil, nil
	}

	// Tokenize
	tokens := s.tokenizer.Encode(text)
	if len(tokens) == 0 {
		return nil, nil
	}

	// Prepare model input
	inputIDs := make([]int64, len(tokens))
	attentionMask := make([]int64, len(tokens))
	for i, t := range tokens {
		inputIDs[i] = int64(t.ID)
		attentionMask[i] = 1
	}

	// Acquire session from pool
	session, err := s.pool.Acquire(ctx)
	if err != nil {
		return nil, err
	}
	defer s.pool.Release(session)

	// Run inference
	logits, err := session.Infer(ctx, inputIDs, attentionMask)
	if err != nil {
		return nil, fmt.Errorf("inference: %w", err)
	}

	// Find boundaries
	var boundaries []int
	for i, logit := range logits {
		if sigmoid(logit) > s.threshold {
			// Map token end position to character position
			if i < len(tokens) {
				boundaries = append(boundaries, tokens[i].End)
			}
		}
	}

	// Split text at boundaries
	if len(boundaries) == 0 {
		return []string{text}, nil
	}

	var sentences []string
	start := 0
	for _, end := range boundaries {
		if end > start && end <= len(text) {
			sentences = append(sentences, text[start:end])
			start = end
		}
	}
	if start < len(text) {
		sentences = append(sentences, text[start:])
	}

	return sentences, nil
}

// Close releases all resources.
func (s *Segmenter) Close() error {
	var errs []error

	if s.pool != nil {
		if err := s.pool.Close(); err != nil {
			errs = append(errs, err)
		}
	}

	if s.tokenizer != nil {
		if err := s.tokenizer.Close(); err != nil {
			errs = append(errs, err)
		}
	}

	if len(errs) > 0 {
		return errors.Join(errs...)
	}
	return nil
}

func sigmoid(x float32) float32 {
	return float32(1.0 / (1.0 + math.Exp(float64(-x))))
}
```

### Step 4: Run tests

```bash
go test -v
```

Expected: PASS (or SKIP if models not available)

### Step 5: Commit

```bash
git add -A
git commit -m "feat: implement Segmenter API

- New() with functional options
- IsComplete() with context support
- Segment() for full sentence splitting
- Session pool integration
- Proper resource cleanup

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 11: CLI Tool

**Files:**
- Create: `cmd/sat-cli/main.go`

### Step 1: Implement CLI

Create `cmd/sat-cli/main.go`:

```go
package main

import (
	"context"
	"flag"
	"fmt"
	"os"
	"strings"

	sat "github.com/jamesainslie/go-sat"
)

func main() {
	modelPath := flag.String("model", "", "Path to ONNX model file")
	tokenizerPath := flag.String("tokenizer", "", "Path to SentencePiece model file")
	threshold := flag.Float64("threshold", 0.025, "Boundary detection threshold")
	mode := flag.String("mode", "complete", "Mode: complete or segment")

	flag.Parse()

	if *modelPath == "" || *tokenizerPath == "" {
		fmt.Fprintln(os.Stderr, "Usage: sat-cli -model MODEL -tokenizer TOKENIZER [OPTIONS] TEXT")
		flag.PrintDefaults()
		os.Exit(1)
	}

	text := strings.Join(flag.Args(), " ")
	if text == "" {
		fmt.Fprintln(os.Stderr, "Error: no text provided")
		os.Exit(1)
	}

	seg, err := sat.New(*modelPath, *tokenizerPath, sat.WithThreshold(float32(*threshold)))
	if err != nil {
		fmt.Fprintf(os.Stderr, "Error creating segmenter: %v\n", err)
		os.Exit(1)
	}
	defer seg.Close()

	ctx := context.Background()

	switch *mode {
	case "complete":
		complete, confidence, err := seg.IsComplete(ctx, text)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error: %v\n", err)
			os.Exit(1)
		}
		fmt.Printf("Text: %q\n", text)
		fmt.Printf("Complete: %v\n", complete)
		fmt.Printf("Confidence: %.4f\n", confidence)

	case "segment":
		sentences, err := seg.Segment(ctx, text)
		if err != nil {
			fmt.Fprintf(os.Stderr, "Error: %v\n", err)
			os.Exit(1)
		}
		fmt.Printf("Text: %q\n", text)
		fmt.Printf("Sentences (%d):\n", len(sentences))
		for i, s := range sentences {
			fmt.Printf("  %d: %q\n", i+1, s)
		}

	default:
		fmt.Fprintf(os.Stderr, "Unknown mode: %s\n", *mode)
		os.Exit(1)
	}
}
```

### Step 2: Build and test

```bash
go build -o sat-cli ./cmd/sat-cli
```

### Step 3: Commit

```bash
git add -A
git commit -m "feat: add CLI tool

- sat-cli for testing and debugging
- Support complete and segment modes
- Configurable threshold

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Task 12: Documentation

**Files:**
- Create: `doc.go`
- Create: `README.md`

### Step 1: Create package documentation

Create `doc.go`:

```go
// Package sat provides sentence boundary detection using wtpsplit/SaT ONNX models.
//
// # Quick Start
//
//	seg, err := sat.New("model.onnx", "tokenizer.model")
//	if err != nil {
//	    log.Fatal(err)
//	}
//	defer seg.Close()
//
//	complete, confidence, err := seg.IsComplete(ctx, "Hello world.")
//	if err != nil {
//	    log.Fatal(err)
//	}
//	fmt.Printf("Complete: %v (confidence: %.2f)\n", complete, confidence)
//
// # Thread Safety
//
// Segmenter is safe for concurrent use. It manages an internal pool of ONNX
// sessions, configurable via WithPoolSize.
//
// # Model Files
//
// Download from HuggingFace:
//   - Model: https://huggingface.co/segment-any-text/sat-1l-sm/resolve/main/model_optimized.onnx
//   - Tokenizer: https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model
package sat
```

### Step 2: Create README

Create `README.md`:

```markdown
# go-sat

Pure Go library for sentence boundary detection using [wtpsplit/SaT](https://github.com/segment-any-text/wtpsplit) ONNX models.

## Installation

```bash
go get github.com/jamesainslie/go-sat
```

**Note:** Requires ONNX Runtime shared library. See [onnxruntime_go](https://github.com/yalue/onnxruntime_go) for installation instructions.

## Quick Start

```go
package main

import (
    "context"
    "fmt"
    "log"

    sat "github.com/jamesainslie/go-sat"
)

func main() {
    seg, err := sat.New("model_optimized.onnx", "sentencepiece.bpe.model")
    if err != nil {
        log.Fatal(err)
    }
    defer seg.Close()

    ctx := context.Background()

    // Check if text is a complete sentence
    complete, confidence, err := seg.IsComplete(ctx, "Hello world.")
    if err != nil {
        log.Fatal(err)
    }
    fmt.Printf("Complete: %v (confidence: %.2f)\n", complete, confidence)

    // Split text into sentences
    sentences, err := seg.Segment(ctx, "Hello world. How are you?")
    if err != nil {
        log.Fatal(err)
    }
    for _, s := range sentences {
        fmt.Println(s)
    }
}
```

## Model Files

Download from HuggingFace:

```bash
# Tokenizer
curl -L -o sentencepiece.bpe.model \
  "https://huggingface.co/xlm-roberta-base/resolve/main/sentencepiece.bpe.model"

# Model (choose one)
# sat-1l-sm (fast, ~400MB)
curl -L -o model_optimized.onnx \
  "https://huggingface.co/segment-any-text/sat-1l-sm/resolve/main/model_optimized.onnx"
```

## Options

```go
seg, err := sat.New(modelPath, tokenizerPath,
    sat.WithThreshold(0.025),     // Boundary detection threshold
    sat.WithPoolSize(4),          // ONNX session pool size
    sat.WithLogger(slog.Default()), // Custom logger
)
```

## CLI Tool

```bash
go install github.com/jamesainslie/go-sat/cmd/sat-cli@latest

sat-cli -model model.onnx -tokenizer tokenizer.model "Hello world."
sat-cli -model model.onnx -tokenizer tokenizer.model -mode segment "Hello. World."
```

## License

MIT
```

### Step 3: Commit

```bash
git add -A
git commit -m "docs: add package documentation and README

- Package doc.go with quick start
- README with installation and usage
- Model download instructions

Co-Authored-By: Claude Opus 4.5 <noreply@anthropic.com>"
```

---

## Summary

| Task | Component | Files |
|------|-----------|-------|
| 0 | Project Setup | go.mod, testdata/, scripts/ |
| 1 | Protobuf Loading | internal/proto/, tokenizer/model.go |
| 2 | Tokenizer Structure | tokenizer/tokenizer.go |
| 3 | Text Normalization | tokenizer/normalize.go |
| 4 | Unigram Viterbi | tokenizer/unigram.go |
| 5 | Golden Validation | tokenizer/tokenizer_test.go |
| 6 | Decode | tokenizer/tokenizer.go |
| 7 | ONNX Session | inference/session.go |
| 8 | Session Pool | inference/pool.go |
| 9 | Errors & Options | errors.go, options.go |
| 10 | Segmenter API | sat.go |
| 11 | CLI Tool | cmd/sat-cli/main.go |
| 12 | Documentation | doc.go, README.md |

**Total commits:** 13 (including initial setup)
